"""
æœ€ç»ˆè¯„ä¼°è„šæœ¬ï¼šä½¿ç”¨mBARTé›¶æ ·æœ¬æ¨¡å‹
åŸºäºå®éªŒç»“æœï¼Œé›¶æ ·æœ¬æ€§èƒ½ï¼ˆ21.64ï¼‰ä¼˜äºæ‰€æœ‰fine-tuningå°è¯•
"""
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import evaluate
import time
from tqdm import tqdm
import json
from pathlib import Path

def main():
    print("=" * 80)
    print("æœ€ç»ˆè¯„ä¼°ï¼šmBART-large-50 é›¶æ ·æœ¬ç¿»è¯‘")
    print("=" * 80)
    print("\nå®éªŒæ€»ç»“:")
    print("  opus-mt é›¶æ ·æœ¬:        BLEU = 19.92")
    print("  opus-mt fine-tuned:    BLEU = 18.47-19.22 (ä¸‹é™)")
    print("  mBART é›¶æ ·æœ¬:          BLEU = 21.64 âœ…")
    print("  mBART fine-tuned:      BLEU = 19.66 (ä¸‹é™)")
    print("\nç»“è®º: é›¶æ ·æœ¬mBARTæ€§èƒ½æœ€ä¼˜ï¼Œfine-tuningåè€Œç ´åæ€§èƒ½\n")
    print("=" * 80)
    
    # åŠ è½½æ¨¡å‹
    model_name = "facebook/mbart-large-50-many-to-many-mmt"
    print(f"\nåŠ è½½æ¨¡å‹: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        src_lang="zh_CN",
        tgt_lang="en_XX"
    )
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    model.eval()
    model.cuda()
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

    # åŠ è½½æ•°æ®
    print("\nåŠ è½½WMT19 zh-enéªŒè¯é›†...")
    wmt19 = load_dataset("wmt19", "zh-en")
    test_data = wmt19["validation"]
    
    num_samples = len(test_data)
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {num_samples}")

    # å‡†å¤‡æ•°æ®
    sources = [ex["zh"] for ex in test_data["translation"]]
    references = [[ex["en"]] for ex in test_data["translation"]]

    # ç¿»è¯‘
    print("\nå¼€å§‹ç¿»è¯‘...")
    predictions = []
    batch_size = 8
    
    start_time = time.time()
    forced_bos_token_id = tokenizer.lang_code_to_id["en_XX"]
    
    for i in tqdm(range(0, len(sources), batch_size)):
        batch = sources[i:i+batch_size]
        inputs = tokenizer(
            batch, 
            return_tensors="pt", 
            padding=True, 
            truncation=True, 
            max_length=128
        ).to("cuda")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=128,
                num_beams=4,
                early_stopping=True,
                forced_bos_token_id=forced_bos_token_id
            )
        
        batch_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        predictions.extend(batch_preds)
    
    runtime = time.time() - start_time

    # è®¡ç®—BLEU
    print("\nè®¡ç®—BLEUåˆ†æ•°...")
    bleu_metric = evaluate.load("sacrebleu")
    bleu_result = bleu_metric.compute(predictions=predictions, references=references)

    # æœ€ç»ˆç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ¯ æœ€ç»ˆè¯„ä¼°ç»“æœ")
    print("=" * 80)
    
    final_metrics = {
        'model': 'facebook/mbart-large-50-many-to-many-mmt',
        'strategy': 'zero-shot (no fine-tuning)',
        'test_dataset': 'WMT19 zh-en validation',
        'test_samples': num_samples,
        'test_bleu': round(bleu_result['score'], 2),
        'test_runtime': round(runtime, 2),
        'test_samples_per_second': round(num_samples / runtime, 2),
        'parameters': '611M',
        'conclusion': 'Best result among all experiments'
    }
    
    for key, value in final_metrics.items():
        print(f"  {key:30s}: {value}")
    
    print("=" * 80)
    
    # ä¿å­˜ç»“æœ
    output_dir = Path("./results")
    output_dir.mkdir(exist_ok=True)
    
    with open(output_dir / "final_results.json", 'w', encoding='utf-8') as f:
        json.dump(final_metrics, f, indent=2, ensure_ascii=False)
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_dir / 'final_results.json'}")
    
    # ä¿å­˜æ ·ä¾‹ç¿»è¯‘
    print("\næ ·ä¾‹ç¿»è¯‘ (å‰10æ¡):")
    print("-" * 80)
    samples_output = []
    for i in range(min(10, len(predictions))):
        sample = {
            'index': i + 1,
            'source': sources[i],
            'prediction': predictions[i],
            'reference': references[i][0]
        }
        samples_output.append(sample)
        print(f"\n[{i+1}]")
        print(f"æºæ–‡: {sources[i]}")
        print(f"é¢„æµ‹: {predictions[i]}")
        print(f"å‚è€ƒ: {references[i][0]}")
        print("-" * 80)
    
    with open(output_dir / "sample_translations.json", 'w', encoding='utf-8') as f:
        json.dump(samples_output, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… æ ·ä¾‹ç¿»è¯‘å·²ä¿å­˜åˆ°: {output_dir / 'sample_translations.json'}")
    print("\n" + "=" * 80)
    print("è¯„ä¼°å®Œæˆï¼")
    print("=" * 80)

if __name__ == "__main__":
    main()
