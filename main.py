import os
# æ¶ˆé™¤è­¦å‘Šä¿¡æ¯
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # æ¶ˆé™¤tokenizerså¹¶è¡Œè­¦å‘Š
os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"  # æ¶ˆé™¤CUDAç¡®å®šæ€§è­¦å‘Š

from pathlib import Path
from dataset import build_dataset, preprocess_data
from model import initialize_model, initialize_tokenizer
from trainer import build_trainer
from utils import not_change_test_dataset, set_random_seeds
from constants import OUTPUT_DIR


def get_latest_checkpoint(output_dir):
    """
    è·å–æœ€æ–°çš„checkpointè·¯å¾„ï¼Œç”¨äºæ–­ç‚¹ç»­è®­ã€‚
    
    Args:
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
    
    Returns:
        str or None: æœ€æ–°checkpointçš„è·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
    """
    output_path = Path(output_dir)
    if not output_path.exists():
        return None
    
    # æŸ¥æ‰¾æ‰€æœ‰checkpointç›®å½•
    checkpoints = [
        d for d in output_path.iterdir() 
        if d.is_dir() and d.name.startswith("checkpoint-")
    ]
    
    if not checkpoints:
        return None
    
    # æŒ‰checkpointç¼–å·æ’åºï¼Œè·å–æœ€æ–°çš„
    checkpoints.sort(key=lambda x: int(x.name.split("-")[-1]))
    latest = checkpoints[-1]
    
    print(f"\nğŸ”„ æ£€æµ‹åˆ°checkpoint: {latest.name}")
    print(f"   å°†ä»æ­¤å¤„ç»§ç»­è®­ç»ƒ...")
    return str(latest)


def main():
    """
    Main function to execute model training and evaluation.
    æ”¯æŒæ–­ç‚¹ç»­è®­ï¼šå¦‚æœæ£€æµ‹åˆ°checkpointï¼Œä¼šè‡ªåŠ¨ä»æœ€æ–°checkpointç»§ç»­è®­ç»ƒã€‚
    """
    # Set random seeds for reproducibility
    set_random_seeds()

    # Initialize tokenizer and model
    model = initialize_model()

    # Initialize tokenizer
    tokenizer = initialize_tokenizer()

    raw_datasets = build_dataset()

    assert not_change_test_dataset(raw_datasets), "You should not change the test dataset"

    # Load and preprocess datasets
    tokenized_datasets = preprocess_data(raw_datasets, tokenizer)

    # Build and train the model
    trainer = build_trainer(
        model=model,
        tokenizer=tokenizer,
        tokenized_datasets=tokenized_datasets,
    )
    
    # æ£€æµ‹æ˜¯å¦æœ‰checkpointå¯ä»¥æ¢å¤
    resume_from_checkpoint = get_latest_checkpoint(OUTPUT_DIR)
    
    if resume_from_checkpoint:
        print(f"âœ… ä»checkpointæ¢å¤è®­ç»ƒ")
    else:
        print(f"ğŸ†• å¼€å§‹æ–°çš„è®­ç»ƒ")
    
    # å¼€å§‹è®­ç»ƒï¼ˆå¦‚æœæœ‰checkpointä¼šè‡ªåŠ¨æ¢å¤ï¼‰
    trainer.train(resume_from_checkpoint=resume_from_checkpoint)

    # Evaluate the model on the test dataset
    test_metrics = trainer.evaluate(
        eval_dataset=tokenized_datasets["test"],
        metric_key_prefix="test",
    )
    print("Test Metrics:", test_metrics)


if __name__ == "__main__":
    main()