"""
é›¶æ ·æœ¬æµ‹è¯•è„šæœ¬ï¼šæµ‹è¯•å¤šä¸ªé«˜çº§ç¿»è¯‘æ¨¡å‹åœ¨ WMT19 zh-en æµ‹è¯•é›†ä¸Šçš„ BLEU åˆ†æ•°
ç”¨äºé€‰æ‹©æœ€ä½³åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒ
"""

import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
import evaluate
from tqdm import tqdm
import gc

# é…ç½®
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TEST_SAMPLES = None  # None è¡¨ç¤ºä½¿ç”¨å®Œæ•´æµ‹è¯•é›† 3981 æ¡
BATCH_SIZE = 8  # æ ¹æ®æ˜¾å­˜è°ƒæ•´

# è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨ï¼ˆéƒ½æ˜¯é«˜ä¸‹è½½é‡çš„çƒ­é—¨æ¨¡å‹ï¼‰
MODELS_TO_TEST = [
    {
        "name": "Helsinki-NLP/opus-mt-zh-en",
        "type": "marianmt",
        "src_lang": None,  # MarianMT ä¸éœ€è¦è®¾ç½®è¯­è¨€ä»£ç 
        "tgt_lang": None,
    },
    {
        "name": "facebook/nllb-200-distilled-600M",
        "type": "nllb",
        "src_lang": "zho_Hans",  # NLLB ä½¿ç”¨ç‰¹æ®Šè¯­è¨€ä»£ç 
        "tgt_lang": "eng_Latn",
    },
    {
        "name": "facebook/m2m100_418M",
        "type": "m2m100",
        "src_lang": "zh",
        "tgt_lang": "en",
    },
    {
        "name": "facebook/mbart-large-50-many-to-many-mmt",
        "type": "mbart",
        "src_lang": "zh_CN",
        "tgt_lang": "en_XX",
    },
    {
        "name": "facebook/mbart-large-50-one-to-many-mmt",
        "type": "mbart",
        "src_lang": "zh_CN",
        "tgt_lang": "en_XX",
    },
]


def load_test_data(num_samples = TEST_SAMPLES):
    """åŠ è½½ WMT19 æµ‹è¯•é›†"""
    wmt19 = load_dataset("wmt19", "zh-en")
    if num_samples is None:
        test_data = wmt19["validation"]  # å®Œæ•´æµ‹è¯•é›†
        print(f"åŠ è½½ WMT19 zh-en å®Œæ•´æµ‹è¯•é›† ({len(test_data)} æ¡)...")
    else:
        test_data = wmt19["validation"].select(range(min(num_samples, len(wmt19["validation"]))))
        print(f"åŠ è½½ WMT19 zh-en æµ‹è¯•é›† (å‰ {num_samples} æ¡)...")
    
    sources = [ex["zh"] for ex in test_data["translation"]]
    references = [[ex["en"]] for ex in test_data["translation"]]
    
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(sources)}")
    print(f"ç¤ºä¾‹è¾“å…¥: {sources[0][:50]}...")
    print(f"ç¤ºä¾‹å‚è€ƒ: {references[0][0][:50]}...")
    return sources, references


def test_model(model_config: dict, sources: list, references: list) -> dict:
    """æµ‹è¯•å•ä¸ªæ¨¡å‹çš„é›¶æ ·æœ¬ç¿»è¯‘æ€§èƒ½"""
    model_name = model_config["name"]
    model_type = model_config["type"]
    
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•æ¨¡å‹: {model_name}")
    print(f"æ¨¡å‹ç±»å‹: {model_type}")
    print(f"{'='*60}")
    
    try:
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # è®¾ç½®æºè¯­è¨€ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if model_type == "nllb":
            tokenizer.src_lang = model_config["src_lang"]
        elif model_type == "mbart":
            tokenizer.src_lang = model_config["src_lang"]
        elif model_type == "m2m100":
            tokenizer.src_lang = model_config["src_lang"]
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32
        ).to(DEVICE)
        model.eval()
        
        # è·å–ç›®æ ‡è¯­è¨€çš„ forced_bos_token_id
        forced_bos_token_id = None
        if model_type == "nllb":
            forced_bos_token_id = tokenizer.convert_tokens_to_ids(model_config["tgt_lang"])
        elif model_type == "mbart":
            forced_bos_token_id = tokenizer.convert_tokens_to_ids(model_config["tgt_lang"])
        elif model_type == "m2m100":
            forced_bos_token_id = tokenizer.get_lang_id(model_config["tgt_lang"])
        
        print(f"forced_bos_token_id: {forced_bos_token_id}")
        
        # æ‰¹é‡ç¿»è¯‘
        predictions = []
        for i in tqdm(range(0, len(sources), BATCH_SIZE), desc="ç¿»è¯‘ä¸­"):
            batch = sources[i:i + BATCH_SIZE]
            
            inputs = tokenizer(
                batch,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=128
            ).to(DEVICE)
            
            with torch.no_grad():
                if forced_bos_token_id is not None:
                    outputs = model.generate(
                        **inputs,
                        forced_bos_token_id=forced_bos_token_id,
                        max_length=128,
                        num_beams=4,
                    )
                else:
                    outputs = model.generate(
                        **inputs,
                        max_length=128,
                        num_beams=4,
                    )
            
            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            predictions.extend(decoded)
        
        # è®¡ç®— BLEU
        metric = evaluate.load("sacrebleu")
        result = metric.compute(predictions=predictions, references=references)
        bleu_score = result["score"]
        
        print(f"\nâœ“ BLEU åˆ†æ•°: {bleu_score:.2f}")
        print(f"ç¤ºä¾‹ç¿»è¯‘: {predictions[0][:80]}...")
        
        # æ¸…ç†æ˜¾å­˜
        del model, tokenizer
        gc.collect()
        if DEVICE == "cuda":
            torch.cuda.empty_cache()
        
        return {
            "model": model_name,
            "bleu": bleu_score,
            "status": "success",
            "sample_output": predictions[0]
        }
        
    except Exception as e:
        print(f"âœ— é”™è¯¯: {str(e)}")
        gc.collect()
        if DEVICE == "cuda":
            torch.cuda.empty_cache()
        return {
            "model": model_name,
            "bleu": 0,
            "status": "failed",
            "error": str(e)
        }


def main():
    print("="*60)
    print("é›¶æ ·æœ¬ç¿»è¯‘æ¨¡å‹æµ‹è¯• (ä¸­æ–‡ â†’ è‹±æ–‡)")
    print(f"è®¾å¤‡: {DEVICE}")
    if DEVICE == "cuda":
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print("="*60)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    sources, references = load_test_data()
    
    # æµ‹è¯•æ¯ä¸ªæ¨¡å‹
    results = []
    for model_config in MODELS_TO_TEST:
        result = test_model(model_config, sources, references)
        results.append(result)
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "="*60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*60)
    print(f"{'æ¨¡å‹åç§°':<45} {'BLEU':>8} {'çŠ¶æ€':>10}")
    print("-"*60)
    
    # æŒ‰ BLEU åˆ†æ•°æ’åº
    results.sort(key=lambda x: x["bleu"], reverse=True)
    
    for r in results:
        status = "âœ“" if r["status"] == "success" else "âœ—"
        print(f"{r['model']:<45} {r['bleu']:>8.2f} {status:>10}")
    
    print("-"*60)
    if results and results[0]["status"] == "success":
        print(f"\nğŸ† æ¨èæ¨¡å‹: {results[0]['model']} (BLEU: {results[0]['bleu']:.2f})")
    
    # ä¸ä¿å­˜æ–‡ä»¶ï¼Œåªæ‰“å°ç»“æœ
    print("\næµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()
