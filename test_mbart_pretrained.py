"""
æµ‹è¯•mBARTé¢„è®­ç»ƒæ¨¡å‹çš„é›¶æ ·æœ¬BLEUåˆ†æ•°
å¿«é€ŸéªŒè¯mBARTæ˜¯å¦æ¯”opus-mtæ›´å¥½
"""
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import evaluate
import time
from tqdm import tqdm

def main():
    # åŠ è½½mBARTæ¨¡å‹
    model_name = "facebook/mbart-large-50-many-to-many-mmt"
    print(f"Loading mBART model: {model_name}")
    print("=" * 60)
    print("âš ï¸  Note: mBART is 611M parameters, loading may take a few minutes...")
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        src_lang="zh_CN",
        tgt_lang="en_XX"
    )
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    model.eval()
    model.cuda()
    
    print("âœ… Model loaded successfully!")

    # åŠ è½½WMT19éªŒè¯é›†ï¼ˆå®Œæ•´å®˜æ–¹éªŒè¯é›†ï¼‰
    print("\nLoading WMT19 zh-en validation set...")
    wmt19 = load_dataset("wmt19", "zh-en")
    test_data = wmt19["validation"]  # ä½¿ç”¨å®Œæ•´éªŒè¯é›†ï¼ˆçº¦4000æ¡ï¼‰
    
    num_samples = len(test_data)
    print(f"Total test samples: {num_samples}")
    print("=" * 60)

    # å‡†å¤‡æ•°æ®
    sources = [ex["zh"] for ex in test_data["translation"]]
    references = [[ex["en"]] for ex in test_data["translation"]]

    # ç¿»è¯‘
    print("\nTranslating with mBART...")
    predictions = []
    batch_size = 8  # mBARTè¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦è¾ƒå°çš„batch
    
    start_time = time.time()
    
    # è®¾ç½®å¼ºåˆ¶çš„BOS tokenä¸ºç›®æ ‡è¯­è¨€
    forced_bos_token_id = tokenizer.lang_code_to_id["en_XX"]
    
    for i in tqdm(range(0, len(sources), batch_size)):
        batch = sources[i:i+batch_size]
        
        # mBART tokenization
        inputs = tokenizer(
            batch, 
            return_tensors="pt", 
            padding=True, 
            truncation=True, 
            max_length=128
        ).to("cuda")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=128,
                num_beams=4,
                early_stopping=True,
                forced_bos_token_id=forced_bos_token_id  # å¼ºåˆ¶è¾“å‡ºè‹±æ–‡
            )
        
        batch_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        predictions.extend(batch_preds)
    
    runtime = time.time() - start_time

    # è®¡ç®—BLEU
    print("\nComputing BLEU score...")
    bleu_metric = evaluate.load("sacrebleu")
    bleu_result = bleu_metric.compute(predictions=predictions, references=references)

    # è¾“å‡ºæ ¼å¼ä¸main.pyä¿æŒä¸€è‡´
    print("\n" + "="*60)
    test_metrics = {
        'test_bleu': bleu_result['score'],
        'test_runtime': runtime,
        'test_samples_per_second': num_samples / runtime,
        'model': 'mBART-large-50 (zero-shot)',
        'note': 'No fine-tuning, 611M parameters'
    }
    print("Test Metrics:", test_metrics)
    print("="*60)
    
    # ä¸opus-mtå¯¹æ¯”
    print("\nğŸ“Š Comparison with opus-mt:")
    print(f"   opus-mt-zh-en (77M):  BLEU = 19.92")
    print(f"   mBART-large (611M):   BLEU = {bleu_result['score']:.2f}")
    print(f"   Improvement:          {bleu_result['score'] - 19.92:+.2f}")
    print("="*60)

    # å±•ç¤ºä¸€äº›æ ·ä¾‹
    print("\næ ·ä¾‹ç¿»è¯‘ (å‰5æ¡)ï¼š")
    print("-" * 60)
    for i in range(min(5, len(predictions))):
        print(f"\n[{i+1}]")
        print(f"æºæ–‡: {sources[i]}")
        print(f"é¢„æµ‹: {predictions[i]}")
        print(f"å‚è€ƒ: {references[i][0]}")
        print("-" * 60)

if __name__ == "__main__":
    main()
