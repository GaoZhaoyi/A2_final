- **目标**：在翻译数据集上对合适的 Hugging Face（HF）模型进行微调，以获得有竞争力的 BLEU 分数。
- **可修改内容**：除了 `main.py`、`evaluation.py` 的部分内容和 `utils.py` 之外，你可以修改代码库中的任何文件。测试集不得修改。
- **可改进方向**：
    - **丰富数据集**：使用更多样化的数据集（测试集不得泄露到训练集中）
    - **基础模型选择**：为你的语言对选择合适的预训练模型
    - **训练流程**：调整超参数（批量大小、学习率（LR）、轮次（epochs）、调度器、标签平滑、梯度累积）
    - **数据处理**：分词长度、过滤、清洗、语言代码、特殊标记
    - **高级 HF 特性**：混合精度（fp16/bf16）、梯度检查点、LoRA/PEFT、更好的数据整理器、调度器选择、早停
- **示例可接受的数据集/模型**
    - **数据集**：wmt14、wmt16、wmt19、opus100、ted_talks_iwslt 等（通过 HF Datasets 获取）
    - **模型**：MarianMT（Helsinki-NLP/opus-mt-xx-yy）、mT5、MBART-50、M2M100、NLLB-200（确保支持你的语言对），甚至大型语言模型（LLMs）等


运用 Hugging Face Hub（Hugging Face 中心库）处理数据集：实现翻译数据集的发现、下载与预处理
从社区资源中加载预训练模型与分词器
利用 transformers（转换器）、datasets（数据集处理）、accelerate（加速训练）及 evaluate（评估）工具构建训练流程
为选定的语言对（简体中文→英文，标注：zh-sim->en）端到端微调翻译模型
使用 BLEU 指标（通过 sacrebleu 工具实现）评估翻译质量并汇报结果
培养调试能力，用于识别并解决 Hugging Face 版本环境下深度学习的常见问题

工作流程分为 5 个文件，具体职责如下：
・数据集获取与预处理（dataset.py）
探索 Hugging Face Datasets 资源库，加载数据集拆分（训练 / 验证 / 测试集），执行数据过滤、映射处理，完成训练集、验证集、测试集的预处理准备
・模型与分词器配置（model.py）
从 Hugging Face Hub 选择合适的基础模型，初始化分词器与模型配置
・训练流程搭建（trainer.py）
配置训练参数（TrainingArguments）、数据整理器（data collators）、评估指标、日志记录及检查点保存机制
・训练执行与监控（main.py）
协调端到端训练与验证流程，包含周期性评估环节
・评估与结果汇报（evaluation.py）
在预留的测试集上计算 BLEU 分数；保存模型产物（artifacts）并生成总结报告

我们将重新运行 main.py 并在固定测试集上进行评估。BLEU（SacreBLEU）是主要指标。
重要注意事项：
无错误执行：你的代码必须无错误运行（且在提供的环境中避免 GPU 内存溢出（OOM））
数据使用正确：不得修改测试集或将其泄露到训练过程中
无个人预训练模型：在 HuggingFace 上加载的预训练模型 “上月下载量” 必须大于 10
性能合理：在所选模型和配置下，需取得有竞争力的 BLEU 分数
运行时间：需在合理时间预算内完成（在港大 GPU 集群上使用单块 GPU时 ≤ 12 小时）
基于 BLEU 的评分（暂定阈值，可能调整）：
BLEU ≥ 25：100%
BLEU ≥ 24：90%
BLEU ≥ 23：80%
BLEU ≥ 22：70%
BLEU ≥ 21：60%
BLEU ≥ 20：50%
BLEU < 20 / 无法复现 / 超时：0%