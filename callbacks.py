"""
Custom callbacks for training
åœ¨evaluationæ—¶åŒæ—¶è®¡ç®—test_bleuä¾›å‚è€ƒ
"""
from transformers import TrainerCallback
import evaluate


class TestBLEUCallback(TrainerCallback):
    """
    åœ¨æ¯æ¬¡evaluationåé¢å¤–è®¡ç®—test_bleu
    è¿™æ ·å¯ä»¥å®æ—¶ç›‘æ§æ¨¡å‹åœ¨å®Œæ•´æµ‹è¯•é›†ä¸Šçš„çœŸå®æ€§èƒ½
    """
    
    def __init__(self, trainer, test_dataset, tokenizer):
        self.trainer = trainer
        self.test_dataset = test_dataset
        self.tokenizer = tokenizer
        self.bleu_metric = evaluate.load("sacrebleu")
    
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        """
        åœ¨æ¯æ¬¡evaluationåè°ƒç”¨
        """
        if metrics is None:
            return
        
        # åªåœ¨æœ‰eval_bleuçš„æ—¶å€™è®¡ç®—test_bleu
        if 'eval_bleu' not in metrics:
            return
        
        print("\n" + "="*70)
        print(f"ğŸ“Š Evaluation at step {state.global_step} (epoch {metrics.get('epoch', 0):.2f}):")
        print(f"   Eval BLEU (2K samples): {metrics['eval_bleu']:.2f}")
        
        # è®¡ç®—test_bleu
        try:
            print(f"   Computing test BLEU (full {len(self.test_dataset)} samples)...")
            
            # ä½¿ç”¨trainerçš„predictæ–¹æ³•
            test_output = self.trainer.predict(self.test_dataset, metric_key_prefix="test")
            test_metrics = test_output.metrics
            
            if 'test_bleu' in test_metrics:
                print(f"   Test BLEU (full set):   {test_metrics['test_bleu']:.2f}")
                print(f"   Difference:             {test_metrics['test_bleu'] - metrics['eval_bleu']:.2f}")
            
        except Exception as e:
            print(f"   Failed to compute test BLEU: {str(e)}")
        
        print("="*70 + "\n")
