"""
Custom callbacks for training
åœ¨evaluationæ—¶åŒæ—¶è®¡ç®—test_bleuä¾›å‚è€ƒï¼Œå¹¶ä¿å­˜åˆ°æ–‡ä»¶
"""
from transformers import TrainerCallback
import evaluate
import csv
import os
from datetime import datetime


class TestBLEUCallback(TrainerCallback):
    """
    åœ¨æ¯æ¬¡evaluationåé¢å¤–è®¡ç®—test_bleu
    è¿™æ ·å¯ä»¥å®æ—¶ç›‘æ§æ¨¡å‹åœ¨å®Œæ•´æµ‹è¯•é›†ä¸Šçš„çœŸå®æ€§èƒ½
    æ‰€æœ‰ç»“æœä¿å­˜åˆ°CSVæ–‡ä»¶ä¾›åç»­æŸ¥çœ‹
    """
    
    def __init__(self, trainer, test_dataset, tokenizer, output_dir="./results"):
        self.trainer = trainer
        self.test_dataset = test_dataset
        self.tokenizer = tokenizer
        self.bleu_metric = evaluate.load("sacrebleu")
        self.output_dir = output_dir
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # CSVæ–‡ä»¶è·¯å¾„
        self.csv_path = os.path.join(output_dir, "training_bleu_history.csv")
        
        # åˆå§‹åŒ–CSVæ–‡ä»¶ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not os.path.exists(self.csv_path):
            with open(self.csv_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'timestamp', 'step', 'epoch', 
                    'eval_loss', 'eval_bleu', 
                    'test_loss', 'test_bleu', 
                    'difference', 'status'
                ])
    
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        """
        åœ¨æ¯æ¬¡evaluationåè°ƒç”¨
        """
        if metrics is None:
            return
        
        # åªåœ¨æœ‰eval_bleuçš„æ—¶å€™è®¡ç®—test_bleu
        if 'eval_bleu' not in metrics:
            return
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        epoch = metrics.get('epoch', 0)
        eval_loss = metrics.get('eval_loss', 0)
        eval_bleu = metrics['eval_bleu']
        
        print("\n" + "="*70)
        print(f"ğŸ“Š Evaluation at step {state.global_step} (epoch {epoch:.2f}):")
        print(f"   Eval BLEU (2K samples): {eval_bleu:.2f}")
        
        # è®¡ç®—test_bleu
        test_bleu = None
        test_loss = None
        difference = None
        status = "success"
        
        try:
            print(f"   Computing test BLEU (full {len(self.test_dataset)} samples)...")
            
            # ä½¿ç”¨trainerçš„predictæ–¹æ³•
            test_output = self.trainer.predict(self.test_dataset, metric_key_prefix="test")
            test_metrics = test_output.metrics
            
            if 'test_bleu' in test_metrics:
                test_bleu = test_metrics['test_bleu']
                test_loss = test_metrics.get('test_loss', None)
                difference = test_bleu - eval_bleu
                
                print(f"   Test BLEU (full set):   {test_bleu:.2f}")
                print(f"   Difference:             {difference:+.2f}")
            
        except Exception as e:
            status = f"failed: {str(e)}"
            print(f"   Failed to compute test BLEU: {str(e)}")
        
        print("="*70 + "\n")
        
        # ä¿å­˜åˆ°CSV
        try:
            with open(self.csv_path, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([
                    timestamp, state.global_step, f"{epoch:.2f}",
                    f"{eval_loss:.4f}" if eval_loss else "",
                    f"{eval_bleu:.2f}" if eval_bleu else "",
                    f"{test_loss:.4f}" if test_loss else "",
                    f"{test_bleu:.2f}" if test_bleu else "",
                    f"{difference:+.2f}" if difference is not None else "",
                    status
                ])
            print(f"âœ… Results saved to: {self.csv_path}")
        except Exception as e:
            print(f"âš ï¸  Failed to save results: {str(e)}")
